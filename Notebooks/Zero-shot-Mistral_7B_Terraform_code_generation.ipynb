{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2febb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch transformers bitsandbytes accelerate peft vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b204041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6680226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786b74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adacee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-18 12:00:11 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 12-18 12:00:11 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0980f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"\"\"\"\"\"\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "MAX_RETRIES = 3\n",
    "SLEEP_DURATION = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9944ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to infer device type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen-Orca/Mistral-7B-OpenOrca\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16240\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:214\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, mm_processor_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    192\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    193\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    213\u001b[0m )\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:561\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m engine_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/vllm/engine/arg_utils.py:873\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBitsAndBytes load format and QLoRA adapter only support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m quantization, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_offload_gb \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU offload space must be non-negative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_offload_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 873\u001b[0m device_config \u001b[38;5;241m=\u001b[39m \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model_config()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mis_multimodal_model:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/vllm/config.py:1081\u001b[0m, in \u001b[0;36mDeviceConfig.__init__\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1081\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to infer device type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# Device type is assigned explicitly\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m=\u001b[39m device\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to infer device type"
     ]
    }
   ],
   "source": [
    "model_id = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "llm = LLM(model=model_id, max_model_len=16240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8569c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6622c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatGPT4oresponse(ques):               \n",
    "    \n",
    "    for _ in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model='gpt-4o',\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": \"{}\".format(ques)},\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            return(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            if _ == MAX_RETRIES - 1:  # Last iteration\n",
    "                error = \"An error occurred: {}\\n\".format(e)\n",
    "                return error\n",
    "            time.sleep(SLEEP_DURATION)  # Sleep before retrying\n",
    "\n",
    "def base_gpt4_model(ques):\n",
    "    for _ in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model='gpt-4-turbo',\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": \"{}\".format(ques)},\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            return(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            if _ == MAX_RETRIES - 1:  # Last iteration\n",
    "                return \"An error occurred: {}\".format(e)\n",
    "            time.sleep(SLEEP_DURATION)  # Sleep before retrying\n",
    "\n",
    "def generate_terraform_code(user_query,cloud_provider):\n",
    "    instruction = \"Generate Terraform code based on the provided requirements.\"\n",
    "    prompt=\"\"\"You are a Terraform consultant with expertise in AWS, Azure, and GCP. Your task is to develop a Terraform configuration based on the provided user query for the specified cloud provider.\n",
    "\n",
    "    - *User Query:* {}\n",
    "    - *Cloud Provider:* {}\n",
    "\n",
    "    This Terraform configuration must include modules for essential components and design the modules to be reusable and easy to maintain.\n",
    "    Adhere to the industry best practices for security and efficiency throughout the configuration. The output Terraform config should be in a structured format that is easy to understand and implement.\"\"\"\n",
    "    updated_prompt=prompt.format(user_query,cloud_provider)\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": updated_prompt}\n",
    "    ]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69a969da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Research_Analysis_Base_data_filtered_new.xlsx')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fab53a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Cloud_provider</th>\n",
       "      <th>user_query</th>\n",
       "      <th>Terraform_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoogleCloudPlatform</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td>#\\n# Copyright 2020 Google LLC\\n#\\n# Licensed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>immutability-io</td>\n",
       "      <td>AWS</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td># DHCP Options\\n\\n## Set Terraform version con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forseti-security</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td>/**\\n * Copyright 2019 Google LLC\\n *\\n * Lice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cncf</td>\n",
       "      <td>AWS</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td># resource \"azurerm_network_security_group\" \"c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>terraform-google-modules</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td>resource \"google_compute_ssl_certificate\" \"def...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Category Cloud_provider  \\\n",
       "0       GoogleCloudPlatform            GCP   \n",
       "1           immutability-io            AWS   \n",
       "2          forseti-security            GCP   \n",
       "3                      cncf            AWS   \n",
       "4  terraform-google-modules            GCP   \n",
       "\n",
       "                                          user_query  \\\n",
       "0  Please generate a Terraform configuration for ...   \n",
       "1  Please generate a Terraform configuration for ...   \n",
       "2  Please generate a Terraform configuration for ...   \n",
       "3  Please generate a Terraform configuration for ...   \n",
       "4  Please generate a Terraform configuration for ...   \n",
       "\n",
       "                                      Terraform_Code  \n",
       "0  #\\n# Copyright 2020 Google LLC\\n#\\n# Licensed ...  \n",
       "1  # DHCP Options\\n\\n## Set Terraform version con...  \n",
       "2  /**\\n * Copyright 2019 Google LLC\\n *\\n * Lice...  \n",
       "3  # resource \"azurerm_network_security_group\" \"c...  \n",
       "4  resource \"google_compute_ssl_certificate\" \"def...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac4f851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe51a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst=[]\n",
    "for index, row in df.iterrows():\n",
    "    user_query=row['user_query']\n",
    "    cloud_provider=row['Cloud_provider']\n",
    "    #print(\"user query is {}\".format(user_query))\n",
    "    try:\n",
    "        updated_prompt=generate_terraform_code(user_query,cloud_provider)\n",
    "        prompt_lst.append(updated_prompt)\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f5253fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nGenerate Terraform code based on the provided requirements.<|im_end|>\\n<|im_start|>user\\nYou are a Terraform consultant with expertise in AWS, Azure, and GCP. Your task is to develop a Terraform configuration based on the provided user query for the specified cloud provider.\\n\\n    - *User Query:* Please generate a Terraform configuration for setting up a Google Cloud Platform (GCP) project with necessary services, a VPC network, subnets, firewall rules, a NAT router, compute instances, instance templates, health checks, an instance group manager, and a global load balancer.\\n    - *Cloud Provider:* GCP\\n\\n    This Terraform configuration must include modules for essential components and design the modules to be reusable and easy to maintain.\\n    Adhere to the industry best practices for security and efficiency throughout the configuration. The output Terraform config should be in a structured format that is easy to understand and implement.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a63e1c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/461 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:56:45 scheduler.py:1099] Sequence group 170 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "WARNING 08-11 13:56:52 scheduler.py:1099] Sequence group 120 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1%|          | 3/461 [00:25<48:24,  6.34s/it, est. speed input: 22.25 toks/s, output: 26.96 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:57:05 scheduler.py:1099] Sequence group 69 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 61/461 [01:28<04:25,  1.51it/s, est. speed input: 131.88 toks/s, output: 498.42 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:58:06 scheduler.py:1099] Sequence group 105 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|█▉        | 92/461 [02:12<08:37,  1.40s/it, est. speed input: 133.10 toks/s, output: 547.89 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:58:51 scheduler.py:1099] Sequence group 126 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  25%|██▍       | 113/461 [02:47<10:09,  1.75s/it, est. speed input: 129.74 toks/s, output: 582.74 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:59:26 scheduler.py:1099] Sequence group 160 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  29%|██▉       | 134/461 [03:43<11:21,  2.08s/it, est. speed input: 115.36 toks/s, output: 554.43 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:00:23 scheduler.py:1099] Sequence group 168 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  32%|███▏      | 147/461 [04:09<10:42,  2.05s/it, est. speed input: 113.28 toks/s, output: 567.38 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:00:48 scheduler.py:1099] Sequence group 190 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  36%|███▌      | 164/461 [04:38<07:20,  1.48s/it, est. speed input: 113.63 toks/s, output: 567.66 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:01:17 scheduler.py:1099] Sequence group 198 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  40%|███▉      | 184/461 [05:08<05:34,  1.21s/it, est. speed input: 114.90 toks/s, output: 582.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:01:47 scheduler.py:1099] Sequence group 232 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  43%|████▎     | 198/461 [05:31<05:47,  1.32s/it, est. speed input: 115.49 toks/s, output: 586.16 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:02:10 scheduler.py:1099] Sequence group 236 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|████▉     | 229/461 [06:06<03:51,  1.00it/s, est. speed input: 120.84 toks/s, output: 605.76 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:02:44 scheduler.py:1099] Sequence group 290 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  53%|█████▎    | 245/461 [06:38<06:21,  1.76s/it, est. speed input: 118.86 toks/s, output: 596.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:03:17 scheduler.py:1099] Sequence group 278 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|█████▊    | 270/461 [07:21<06:07,  1.92s/it, est. speed input: 118.27 toks/s, output: 597.11 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:04:00 scheduler.py:1099] Sequence group 304 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  63%|██████▎   | 292/461 [08:02<04:25,  1.57s/it, est. speed input: 117.00 toks/s, output: 588.99 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:04:42 scheduler.py:1099] Sequence group 316 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  67%|██████▋   | 307/461 [08:28<03:42,  1.44s/it, est. speed input: 116.88 toks/s, output: 605.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:05:07 scheduler.py:1099] Sequence group 357 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  72%|███████▏  | 334/461 [09:12<02:54,  1.38s/it, est. speed input: 117.09 toks/s, output: 605.70 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:05:50 scheduler.py:1099] Sequence group 379 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|███████▌  | 351/461 [09:39<02:52,  1.57s/it, est. speed input: 117.26 toks/s, output: 604.18 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:06:22 scheduler.py:1099] Sequence group 379 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  79%|███████▉  | 366/461 [10:23<05:40,  3.58s/it, est. speed input: 113.82 toks/s, output: 592.69 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:07:02 scheduler.py:1099] Sequence group 388 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  82%|████████▏ | 380/461 [10:57<03:14,  2.40s/it, est. speed input: 112.08 toks/s, output: 595.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:07:43 scheduler.py:1099] Sequence group 408 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  86%|████████▋ | 398/461 [11:47<01:33,  1.49s/it, est. speed input: 109.07 toks/s, output: 592.69 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:08:30 scheduler.py:1099] Sequence group 430 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|█████████ | 419/461 [12:36<01:07,  1.61s/it, est. speed input: 107.49 toks/s, output: 592.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:09:15 scheduler.py:1099] Sequence group 460 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 461/461 [14:10<00:00,  1.84s/it, est. speed input: 105.40 toks/s, output: 588.84 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=2500, stop=['[/SYS]', \"<s>\", \"</s>\",\"<|im_end|>\"])\n",
    "outputs= llm.generate(prompt_lst, sampling_params)\n",
    "terraform_code = [output.outputs[0].text.strip() for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc49f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Generated_Tearraform_Code_Mistral_7B']=terraform_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8faf7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terraform {\n",
      "  required_providers {\n",
      "    google = {\n",
      "      source = \"hashicorp/google\"\n",
      "      version = \"4.51.0\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "provider \"google\" {\n",
      "  project = \"your-project-id\"\n",
      "  region  = \"your-region\"\n",
      "}\n",
      "\n",
      "resource \"random_id\" \"suffix\" {\n",
      "  byte_length = 4\n",
      "}\n",
      "\n",
      "module \"ssl_certificate\" {\n",
      "  source = \"your-module-path/ssl-certificate\"\n",
      "\n",
      "  name_prefix = \"your-prefix\"\n",
      "  description = \"your-description\"\n",
      "  random_suffix = random_id.suffix.dec\n",
      "}\n",
      "\n",
      "output \"ssl_certificate_arn\" {\n",
      "  value = module.ssl_certificate.ssl_certificate_arn\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[4]['Generated_Tearraform_Code_Mistral_7B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32efa4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Cloud_provider</th>\n",
       "      <th>user_query</th>\n",
       "      <th>Terraform_Code</th>\n",
       "      <th>Generated_Tearraform_Code_Mistral_7B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoogleCloudPlatform</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td>#\\n# Copyright 2020 Google LLC\\n#\\n# Licensed ...</td>\n",
       "      <td># Google Cloud Platform (GCP) Terraform Config...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>immutability-io</td>\n",
       "      <td>AWS</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td># DHCP Options\\n\\n## Set Terraform version con...</td>\n",
       "      <td>terraform {\\n  required_providers {\\n    aws =...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forseti-security</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td>/**\\n * Copyright 2019 Google LLC\\n *\\n * Lice...</td>\n",
       "      <td># Terraform Configuration for Google Cloud Pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cncf</td>\n",
       "      <td>AWS</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td># resource \"azurerm_network_security_group\" \"c...</td>\n",
       "      <td>#---------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>terraform-google-modules</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Please generate a Terraform configuration for ...</td>\n",
       "      <td>resource \"google_compute_ssl_certificate\" \"def...</td>\n",
       "      <td>terraform {\\n  required_providers {\\n    googl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Category Cloud_provider  \\\n",
       "0       GoogleCloudPlatform            GCP   \n",
       "1           immutability-io            AWS   \n",
       "2          forseti-security            GCP   \n",
       "3                      cncf            AWS   \n",
       "4  terraform-google-modules            GCP   \n",
       "\n",
       "                                          user_query  \\\n",
       "0  Please generate a Terraform configuration for ...   \n",
       "1  Please generate a Terraform configuration for ...   \n",
       "2  Please generate a Terraform configuration for ...   \n",
       "3  Please generate a Terraform configuration for ...   \n",
       "4  Please generate a Terraform configuration for ...   \n",
       "\n",
       "                                      Terraform_Code  \\\n",
       "0  #\\n# Copyright 2020 Google LLC\\n#\\n# Licensed ...   \n",
       "1  # DHCP Options\\n\\n## Set Terraform version con...   \n",
       "2  /**\\n * Copyright 2019 Google LLC\\n *\\n * Lice...   \n",
       "3  # resource \"azurerm_network_security_group\" \"c...   \n",
       "4  resource \"google_compute_ssl_certificate\" \"def...   \n",
       "\n",
       "                Generated_Tearraform_Code_Mistral_7B  \n",
       "0  # Google Cloud Platform (GCP) Terraform Config...  \n",
       "1  terraform {\\n  required_providers {\\n    aws =...  \n",
       "2  # Terraform Configuration for Google Cloud Pub...  \n",
       "3  #---------------------------------------------...  \n",
       "4  terraform {\\n  required_providers {\\n    googl...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74ee780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluvate_terraform_prompt=\"\"\"You are an expert Terraform developer specializing in infrastructure as code for cloud environments. I need you to evaluate the given Generated Terraform codes based on a user query and Golden Terraform code to determine whether the generated Terraform codes are correct or not. For each Terraform code, ensure the following:\n",
    "Correctness: Verify if the code meets the user’s requirements as described in the query.\n",
    "Best Practices: Check if the code follows best practices for the chosen cloud provider.\n",
    "Completeness: Ensure all necessary resources and configurations are included.\n",
    "\n",
    "User query :{}\n",
    "Golden Terraform Code : {}\n",
    "Generated terraform Code : {}\n",
    "The output should contain whether the generated code is  correct or not for the user query and Golden Terraform code , only validate it logically and functionally \n",
    "example output :  Correct:Yes/No, Reason : <Add reason>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a5665b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Research_Analysis_Base_data_filtered_new_Mistral_7B.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa244ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to determine the cloud provider\n",
    "def evaluvate_terraform(inp):\n",
    "    golden=inp['Terraform_Code']\n",
    "    user_query=inp['user_query']\n",
    "    generated=inp['Generated_Tearraform_Code_Mistral_7B']\n",
    "    updated_prompt=evaluvate_terraform_prompt.format(user_query,golden,generated)\n",
    "    res=chatGPT4oresponse(updated_prompt)\n",
    "    return (res)\n",
    "\n",
    "# Apply the function only where Cloud_provider is 'NVF'\n",
    "df['Validation_Result'] = df.apply(evaluvate_terraform,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91c4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
